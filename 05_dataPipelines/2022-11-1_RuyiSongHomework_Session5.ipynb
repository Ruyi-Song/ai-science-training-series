{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNzARRfql5Gd06r8RsLPOpa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import sys, os\n","import time,math\n","\n","# This limits the amount of memory used:\n","os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n","os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_auto_jit=2\"\n","# This control parallelism in Tensorflow\n","parallel_threads = 128\n","# This controls how many batches to prefetch\n","prefetch_buffer_size = 8 # tf.data.AUTOTUNE\n","os.environ['OMP_NUM_THREADS'] = str(parallel_threads)\n","num_parallel_readers = parallel_threads\n","\n","# how many training steps to take during profiling\n","num_steps = 10\n","use_profiler = True"],"metadata":{"id":"bKgNn93CS9LC","executionInfo":{"status":"ok","timestamp":1667317374437,"user_tz":240,"elapsed":13,"user":{"displayName":"宋汝怿","userId":"14175445932911205844"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"CAwIzoBIS27b","executionInfo":{"status":"ok","timestamp":1667317383838,"user_tz":240,"elapsed":6461,"user":{"displayName":"宋汝怿","userId":"14175445932911205844"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.python.profiler import trace"]},{"cell_type":"code","source":["#########################################################################\n","# Here's the Residual layer from the first half again:\n","#########################################################################\n","class ResidualLayer(tf.keras.Model):\n","\n","    def __init__(self, n_filters):\n","        # tf.keras.Model.__init__(self)\n","        super(ResidualLayer, self).__init__()\n","\n","        self.conv1 = tf.keras.layers.Conv2D(\n","            filters     = n_filters,\n","            kernel_size = (3,3),\n","            padding     = \"same\"\n","        )\n","\n","        self.norm1 = tf.keras.layers.BatchNormalization()\n","\n","        self.conv2 = tf.keras.layers.Conv2D(\n","            filters     = n_filters,\n","            kernel_size = (3,3),\n","            padding     = \"same\"\n","        )\n","\n","        self.norm2 = tf.keras.layers.BatchNormalization()\n","\n","    def call(self, inputs):\n","\n","        x = inputs\n","\n","        output1 = self.norm1(self.conv1(inputs))\n","\n","        output1 = tf.keras.activations.relu(output1)\n","\n","        output2 = self.norm2(self.conv2(output1))\n","\n","        return tf.keras.activations.relu(output2 + x)"],"metadata":{"id":"VNf6J6_lThao","executionInfo":{"status":"ok","timestamp":1667317395808,"user_tz":240,"elapsed":211,"user":{"displayName":"宋汝怿","userId":"14175445932911205844"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#########################################################################\n","# Here's layer that does a spatial downsampling:\n","#########################################################################\n","class ResidualDownsample(tf.keras.Model):\n","\n","    def __init__(self, n_filters):\n","        # tf.keras.Model.__init__(self)\n","        super(ResidualDownsample, self).__init__()\n","\n","        self.conv1 = tf.keras.layers.Conv2D(\n","            filters     = n_filters,\n","            kernel_size = (3,3),\n","            padding     = \"same\",\n","            strides     = (2,2)\n","        )\n","\n","        self.identity = tf.keras.layers.Conv2D(\n","            filters     = n_filters,\n","            kernel_size = (1,1),\n","            strides     = (2,2),\n","            padding     = \"same\"\n","        )\n","\n","        self.norm1 = tf.keras.layers.BatchNormalization()\n","\n","        self.conv2 = tf.keras.layers.Conv2D(\n","            filters     = n_filters,\n","            kernel_size = (3,3),\n","            padding     = \"same\"\n","        )\n","\n","        self.norm2 = tf.keras.layers.BatchNormalization()\n","\n","    @tf.function\n","    def call(self, inputs):\n","\n","        x = self.identity(inputs)\n","        output1 = self.norm1(self.conv1(inputs))\n","        output1 = tf.keras.activations.relu(output1)\n","\n","        output2 = self.norm2(self.conv2(output1))\n","\n","        return tf.keras.activations.relu(output2 + x)"],"metadata":{"id":"lYtH9xxyThyH","executionInfo":{"status":"ok","timestamp":1667317398364,"user_tz":240,"elapsed":169,"user":{"displayName":"宋汝怿","userId":"14175445932911205844"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#########################################################################\n","# Armed with that, let's build ResNet (this particular one is called ResNet34)\n","#########################################################################\n","\n","class ResNet34(tf.keras.Model):\n","\n","    def __init__(self):\n","        super(ResNet34, self).__init__()\n","\n","        self.conv_init = tf.keras.Sequential([\n","            tf.keras.layers.Conv2D(\n","                filters     = 64,\n","                kernel_size = (7,7),\n","                strides     = (2,2),\n","                padding     = \"same\",\n","                use_bias    = False\n","            ),\n","            tf.keras.layers.BatchNormalization(),\n","            tf.keras.layers.ReLU(),\n","            tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding=\"same\")\n","\n","        ])\n","\n","        self.residual_series_1 = tf.keras.Sequential([\n","            ResidualLayer(64),\n","            ResidualLayer(64),\n","            ResidualLayer(64),\n","        ])\n","\n","        # Increase the number of filters:\n","        self.downsample_1 = ResidualDownsample(128)\n","\n","        self.residual_series_2 = tf.keras.Sequential([\n","            ResidualLayer(128),\n","            ResidualLayer(128),\n","            ResidualLayer(128),\n","        ])\n","\n","        # Increase the number of filters:\n","        self.downsample_2 = ResidualDownsample(256)\n","\n","        self.residual_series_3 = tf.keras.Sequential([\n","            ResidualLayer(256),\n","            ResidualLayer(256),\n","            ResidualLayer(256),\n","            ResidualLayer(256),\n","            ResidualLayer(256),\n","        ])\n","\n","        # Increase the number of filters:\n","        self.downsample_3 = ResidualDownsample(512)\n","\n","\n","        self.residual_series_4 = tf.keras.Sequential([\n","            ResidualLayer(512),\n","            ResidualLayer(512),\n","        ])\n","\n","        self.final_pool = tf.keras.layers.AveragePooling2D(\n","            pool_size=(8,8)\n","        )\n","\n","        self.flatten = tf.keras.layers.Flatten()\n","        self.classifier = tf.keras.layers.Dense(1000)\n","\n","    @tf.function\n","    def call(self, inputs):\n","\n","        x = self.conv_init(inputs)\n","\n","        x = self.residual_series_1(x)\n","\n","\n","        x = self.downsample_1(x)\n","\n","\n","        x = self.residual_series_2(x)\n","\n","        x = self.downsample_2(x)\n","\n","        x = self.residual_series_3(x)\n","\n","        x = self.downsample_3(x)\n","\n","\n","        x = self.residual_series_4(x)\n","\n","\n","        x = self.final_pool(x)\n","\n","        x = self.flatten(x)\n","\n","        logits = self.classifier(x)\n","\n","        return logits"],"metadata":{"id":"H5xD93rCVVyi","executionInfo":{"status":"ok","timestamp":1667317403368,"user_tz":240,"elapsed":258,"user":{"displayName":"宋汝怿","userId":"14175445932911205844"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["@tf.function()\n","def calculate_accuracy(logits, labels):\n","    # We calculate top1 accuracy only here:\n","    selected_class = tf.argmax(logits, axis=1)\n","\n","    correct = tf.cast(selected_class, tf.float32) == tf.cast(labels, tf.float32)\n","\n","    return tf.reduce_mean(tf.cast(correct, tf.float32))\n","\n","\n","@tf.function()\n","def calculate_loss(logits, labels):\n","    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, logits)\n","    return tf.reduce_mean(loss)\n"],"metadata":{"id":"QXFdDuptVr8k","executionInfo":{"status":"ok","timestamp":1667317407238,"user_tz":240,"elapsed":4,"user":{"displayName":"宋汝怿","userId":"14175445932911205844"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["@tf.function()\n","def training_step(network, optimizer, images, labels):\n","    with tf.GradientTape() as tape:\n","        logits = network(images)\n","        loss = calculate_loss(logits, labels)\n","\n","    gradients = tape.gradient(loss, network.trainable_variables)\n","\n","    optimizer.apply_gradients(zip(gradients, network.trainable_variables))\n","\n","    accuracy = calculate_accuracy(logits, labels)\n","\n","    return loss, accuracy\n","\n","@trace.trace_wrapper('train_epoch')\n","def train_epoch(i_epoch, step_in_epoch, train_ds, val_ds, network, optimizer, BATCH_SIZE, checkpoint):\n","    # Here is our training loop!\n","\n","    steps_per_epoch = int(1281167 / BATCH_SIZE)\n","    steps_validation = int(50000 / BATCH_SIZE)\n","\n","    # added for profiling\n","    if use_profiler:\n","        print('start profiler')\n","        tf.profiler.experimental.start('logdir/m%03d_w%02d_p%02d' % (parallel_threads,num_parallel_readers,prefetch_buffer_size))\n","    \n","    start = time.time()\n","    i = 0\n","    sum = 0.\n","    sum2 = 0.\n","    for train_images, train_labels in train_ds.take(steps_per_epoch):\n","        if step_in_epoch > steps_per_epoch: break\n","        else: step_in_epoch.assign_add(1)\n","\n","        # Peform the training step for this batch\n","        loss, acc = training_step(network, optimizer, train_images, train_labels)\n","        end = time.time()\n","        images_per_second = BATCH_SIZE / (end - start)\n","        if i > 0: # skip the first measurement because it includes compile time\n","            sum += images_per_second\n","            sum2 += images_per_second * images_per_second\n","        print(f\"Finished step {step_in_epoch.numpy()} of {steps_per_epoch} in epoch {i_epoch.numpy()},loss={loss:.3f}, acc={acc:.3f} ({images_per_second:.3f} img/s).\")\n","        start = time.time()\n","        # added for profiling to stop after some steps\n","        i += 1\n","        if i > num_steps and use_profiler: break\n","    \n","    # added for profiling to stop after some steps\n","    if use_profiler:\n","        print('stop profiler')\n","        i = i - 1\n","        mean_rate = sum / i\n","        stddev_rate = math.sqrt( sum2/i - mean_rate * mean_rate )\n","        print(f'mean image/s = {mean_rate:8.2f}   standard deviation: {stddev_rate:8.2f}')\n","        tf.profiler.experimental.stop()\n","        sys.exit(0)\n","\n","    # Save the network after every epoch:\n","    checkpoint.save(\"resnet34/model\")\n","    \n","    # Compute the validation accuracy:\n","    mean_accuracy = None\n","    for val_images, val_labels in val_ds.take(steps_validation):\n","        logits = network(val_images)\n","        accuracy = calculate_accuracy(logits, val_labels)\n","        if mean_accuracy is None:\n","            mean_accuracy = accuracy\n","        else:\n","            mean_accuracy += accuracy\n","\n","    mean_accuracy /= steps_validation\n","\n","    print(f\"Validation accuracy after epoch {i_epoch.numpy()}: {mean_accuracy:.4f}.\")\n"],"metadata":{"id":"8SeinLrAVxZV","executionInfo":{"status":"ok","timestamp":1667317410348,"user_tz":240,"elapsed":176,"user":{"displayName":"宋汝怿","userId":"14175445932911205844"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# @trace.trace_wrapper('prepare_data_loader')\n","def prepare_data_loader(BATCH_SIZE):\n","\n","    tf.config.threading.set_inter_op_parallelism_threads(parallel_threads)\n","    tf.config.threading.set_intra_op_parallelism_threads(parallel_threads)\n","    print('threading set: ',tf.config.threading.get_inter_op_parallelism_threads(),tf.config.threading.get_intra_op_parallelism_threads())\n","\n","    print(\"Parameters set, preparing dataloading\")\n","    #########################################################################\n","    # Here's the part where we load datasets:\n","    import json\n","\n","\n","    # What's in this function?  Tune in next week ...\n","    from ilsvrc_dataset import get_datasets\n","\n","\n","    class FakeHvd:\n","\n","        def size(self): return 1\n","\n","        def rank(self): return 0\n","\n","\n","    with open(\"ilsvrc.json\", 'r') as f:\n","        config = json.load(f)\n","\n","    config['data']['batch_size'] = BATCH_SIZE\n","    config['data']['num_parallel_readers'] = num_parallel_readers\n","    config['data']['prefetch_buffer_size'] = prefetch_buffer_size \n","\n","    print(json.dumps(config, indent=4))\n","\n","    config['hvd'] = FakeHvd()\n","\n","    train_ds, val_ds = get_datasets(config)\n","\n","    options = tf.data.Options()\n","    options.threading.private_threadpool_size = parallel_threads\n","    train_ds = train_ds.with_options(options)\n","    val_ds = val_ds.with_options(options)\n","\n","    print(\"Datasets ready, creating network.\")\n","    #########################################################################\n","\n","    return train_ds, val_ds"],"metadata":{"id":"3iegwFwMVxsr","executionInfo":{"status":"ok","timestamp":1667317414403,"user_tz":240,"elapsed":219,"user":{"displayName":"宋汝怿","userId":"14175445932911205844"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def main():\n","    #########################################################################\n","    # Here's some configuration:\n","    #########################################################################\n","    BATCH_SIZE = 256\n","    N_EPOCHS = 10\n","\n","    train_ds, val_ds = prepare_data_loader(BATCH_SIZE)\n","\n","\n","    example_images, example_labels = next(iter(train_ds.take(1)))\n","\n","\n","    print(\"Initial Image size: \", example_images.shape)\n","    network = ResNet34()\n","\n","    output = network(example_images)\n","    print(\"output shape:\", output.shape)\n","\n","    print(network.summary())\n","\n","    epoch = tf.Variable(initial_value=tf.constant(0, dtype=tf.dtypes.int64), name='epoch')\n","    step_in_epoch = tf.Variable(\n","        initial_value=tf.constant(0, dtype=tf.dtypes.int64),\n","        name='step_in_epoch')\n","\n","\n","    # We need an optimizer.  Let's use Adam:\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n","\n","    checkpoint = tf.train.Checkpoint(\n","        network       = network,\n","        optimizer     = optimizer,\n","        epoch         = epoch,\n","        step_in_epoch = step_in_epoch)\n","\n","    # Restore the model, if possible:\n","    latest_checkpoint = tf.train.latest_checkpoint(\"resnet34/\")\n","    if latest_checkpoint:\n","        checkpoint.restore(latest_checkpoint)\n","\n","    while epoch < N_EPOCHS:\n","        train_epoch(epoch, step_in_epoch, train_ds, val_ds, network, optimizer, BATCH_SIZE, checkpoint)\n","        epoch.assign_add(1)\n","        step_in_epoch.assign(0)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":532},"id":"mEF-PKMOWy2l","executionInfo":{"status":"error","timestamp":1667317422531,"user_tz":240,"elapsed":632,"user":{"displayName":"宋汝怿","userId":"14175445932911205844"}},"outputId":"1fd14254-4de3-4047-f774-a268fe9e143f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["threading set:  128 128\n","Parameters set, preparing dataloading\n"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-d68814921bc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-d68814921bc0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mN_EPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-d0c55adbfc39>\u001b[0m in \u001b[0;36mprepare_data_loader\u001b[0;34m(BATCH_SIZE)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# What's in this function?  Tune in next week ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0milsvrc_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ilsvrc_dataset'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]}]}